---
title: "Summary report of the job files"
author: 'Author: Mario Antonioletti (mario@epcc.ed.ac.uk)'
date: '`r format(Sys.time(), "%d %B, %Y")`'
output:
  html_document:
    fig_width: 9
    fig_height: 5
    dpi: 144
    fig_caption: true
    number_sections: yes
    theme: journal
    toc: yes
  pdf_document:
    fig_caption: yes
    number_sections: yes
    toc: yes
---

<!-- set up defaults for the R chunks in the document       -->
<!-- see http://yihui.name/knitr/options/ for documentation -->
```{r setup, include=FALSE}

# Global options for knitr
knitr::opts_chunk$set(cache=TRUE,echo=FALSE, warning=FALSE, message=FALSE,
                      comment=NA)

```

```{r load_libraries}
# Required libraries
library(pander)    # Use this for creating tables 
library(scales)    # Use this for the percent function
library(ggplot2)   # Use this for plotting
library(ggmap)     # Plot maps
library(NLP)       # Required by the tm library
library(tm)        # Text mining
library(slam)      # Algorithms for sparse matrices

# Set global options for pander
panderOptions('table.split.table', Inf)     # Do not split tables
panderOptions('table.style', 'rmarkdown')   # Output in Rmarkdown

```

<!-- read in the data to be used for processing -->
```{r read_data}

# Set the working directory to be where this file is located - you can
# add a line for your hostname is and only execute that. File paths are
# given relative to this directory.
hostname<-Sys.info()["nodename"]

# Set the working directory. This should be changed to point
# to the directory that contains this file in your system.
if(hostname == "mbp-ma.local"){
  setwd("/Users/mario/jobs-analysis/scripts") # Mario's Apple Macbook
}else if(hostname =="localhost.localdomain"){
  setwd("/home/mario/jobs-analysis/scripts")   # Mario's home Linux desktop
}else if(hostname =="gavin-VirtualBox"){
  setwd("/home/gavin/jobs-analysis/scripts")   # Gavins Linux desktop
}else if(hostname =="MARIO-PC"){
  setwd("C:/Users/mario/Documents/jobs-analysis/scripts")   # Mario's home Windows desktop
}else{
  print("You need to set your current working directory.")
}

# Obtain functions to read and clean data.
source("R/filestuff.R")

# Read in the data into a data frame.
dat<-readJobsFile("../data/jobs.csv")

```

# Overview of the jobs data

## Summary

A summary of how this document can be generated is given in Appendix A.
A description of the terms used is given in Appendix B.

An overview data of the current job data:

```{r data_summary}

# Work out some stats
n  <- length(dat$JobId)             # Total number of jobs
s0 <- sum(dat$SoftwareJob=="1")     # Jobs identified as software jobs.
s1 <- sum(dat$InUK,na.rm=TRUE)      # Jobs in the UK.
s2 <- sum(dat$InUK=="1" & dat$SoftwareJob=="1", na.rm=TRUE) # Software jobs in the UK.
s3 <- sum(dat$JobRef!="" & duplicated(dat$JobRef)) # Number of replicated jobs.

# Get dates jobs were placed on
d1 <- dat$PlacedOn

# Create a matrix with the stats - this is to facilitate printing out
# the values as table using the pander library.
m<-matrix(data=c(
                   n,"-",
                   paste0(length(unique(d1)),"/",max(d1)-min(d1)," days"), "-",
                   s0,percent(s0/n),
                   s1,percent(s1/n),
                   s2,percent(s2/n),
                   s3,percent(s3/n)
                 ),
                 ncol=2, byrow=TRUE,
          dimnames=list(c("Number of jobs downloaded",
                          "Number of days data collected/coverage period",
                          "Software jobs",
                          "Jobs in the UK",
                          "UK Software Jobs",
                          "Number of job reposts"),
                        c("Number","Percentage")))

# Use pander to produce the table
pander(m,style="rmarkdown",
       justify=c("left","left","left"),split.table=Inf)

# Remove variables we do not need any more
rm(m,s0,s1,s2)
```

Notes:

* Number of days for which data is collected is smaller than the
  coverage period because data is only on week days. Also, when the
  scraping was started this picked up jobs that had already been 
  published.
* Currently jobs identified as **Software Jobs** needs revision 
  (see [Appendix B](#identifying-software-jobs)).
* The number of job reposts counts the number of jobs that have
  an available job reference but which is repeated. It appears that
  institutions will repost the same job more than once to generate 
  more traffic to their own web site. These jobs should probably be
  removed from the analysis - not currently being done.
  
## Job downloads per day

Jobs are downloaded at 5am each Monday to Friday. The first
batch of jobs were downloaded on the 10th September 2014. The 
initial scrape also captured jobs that had already been published.

In the diagram below you can see the slight banding produced by the missing 
weekends and over the Christmas period. It also appears that there is a slight 
trend to jobs being advertised at the end of each month.

```{r jobs_downloaded_perday}

# Plot the number of jobs downloaded
qplot(d1,geom="bar",colour=I("red"), binwidth=1,
      ylab="Number of Jobs",xlab="Date") +
      theme(legend.position="none")
```

The coverage period of job downloads stretches over `r
length(unique(d1))` days.  The average number of job downloads per day
is `r round(length(d1)/length(unique(d1)),2)` - this only counts the
days when jobs are downloaded, i.e. over workdays. The actual time
period covered from when the first job was placed to the most current
job is `r max(d1)-min(d1)` days.

The same plot only showing identified UK software jobs:

```{r sw_jobs_downloaded_perday}

# Get dates jobs were placed on
d1s <- dat$PlacedOn[dat$SoftwareJob == 1 & dat$InUK == 1]

# Plot the number of jobs downloaded
qplot(d1s,geom="bar",colour=I("blue"),binwidth=1,
      ylab="Number of UK Software Jobs",xlab="Date") +
      theme(legend.position="none")
```

The average number of download of UK software jobs is
`r round(length(d1s)/length(unique(d1s)),2)`.

## Application time

The plot below depicts the distribution of the number of days that a
job is open to an applicant, i.e. the date the job closes - the date
the job was placed on.

```{r application_time}

# Get the job closing dates.
d2 <- dat$Closes

# Calculate the number of days the job is open.
d3 <- as.numeric(na.omit(d2-d1))

qplot(d3,geom="bar",colour=I("red"),binwidth=1,xlab ="Days job is open",
     ylab="Number of Jobs",xlim=c(0,100)) + theme(legend.position="none")

# Remove variables that are no longer required
rm(d2,d3)

```

and for UK software jobs:

```{r sw_application_time}

# Get the job closing dates.
d2 <- dat$Closes[dat$SoftwareJob == 1 & dat$InUK == 1]

d3 <- as.numeric(na.omit(d2-d1s))

# Calculate the number of days the job is open.
qplot(d3,geom="bar",colour=I("blue"),binwidth=1,xlab ="Days job is open",
     ylab="Number of Jobs",xlim=c(0,100)) + theme(legend.position="none")

# Remove variables that are no longer required
rm(d2,d3)

```


## Cumulative job downloads

For all jobs:

```{r cumulative_job_downloads}

# Cumulative sum of frequency plot
tab <- cumsum(table(d1))

# Create a data frame for ggplot
d<-data.frame(freq=as.numeric(tab),date=as.Date(names(tab)),
              stringsAsFactors = FALSE)

# Produce a bar plot
ggplot(data=d,aes(x=date,y=freq,col="red")) +
  stat_identity(geom="bar") +
  theme(legend.position="none",text = element_text(size=10),
        axis.text.x = element_text(angle=90, vjust=1)) + 
  ylab("Total Number of Jobs Downloaded") +
  geom_vline(xintercept=as.numeric(as.Date("2014-09-10")),colour="blue") +
  geom_text(aes(x=as.Date("2014-08-30"),y=60000,angle=90,size=10,
                label="Start of data collection"),colour="blue") +
  xlab("Dates")

# Unique days
d<-sort(unique(d1))
tab<-tab[d>as.Date("10/09/2014","%d/%m/%Y")]
d<-d[d>as.Date("10/09/2014","%d/%m/%Y")]

# Try doing a linear model - it's getting a good gradient for the 
# line but the y intercept is crazy wrong. The time series aspect 
# is causing it problems. This is probably not the way to go to get
# a linear model of the system.
l<-lm(tab ~ d - 1)
c<-coef(l)

# Clean up variables
rm(d1,tab)
```

For UK software jobs only:

```{r cumulative_sw_job_downloads}

# Cumulative sum of the frequency plot.
tab<-cumsum(table(d1s))

# Create a data frame for ggplot
d<-data.frame(freq=as.numeric(tab),date=as.Date(names(tab)),
    stringsAsFactors = FALASE)

# Produce a bar plot
ggplot(data=d,aes(x=date,y=freq,col="blue")) +
  stat_identity(geom="bar",col="blue") +
  theme(legend.position="none",text = element_text(size=10),
        axis.text.x = element_text(angle=90, vjust=1)) + 
  ylab("Total Number of Jobs Downloaded") +
  geom_vline(xintercept=as.numeric(as.Date("2014-09-10")),colour="red") +
  geom_text(aes(x=as.Date("2014-08-30"),y=4000,angle=90,size=10,
                label="Start of data collection"),colour="red") +
  xlab("Dates")

# Clean up variables
rm(d1s,tab)
```

# Salaries

## Possible issues

Enumerate any possible issues that impact on the interpretation of
these plots:

* Not separating out hourly rates. This applies to all jobs.

## Minimum and maximum

Plot the minimum and maximum salary distributions. Here we only use UK
jobs, i.e. those that have the `InUK` field set to 1 and salaries that
begin with a pound (£) sign:

```{r minmax_salary_distributions}

# Only take UK-based jobs
salary <- dat$Salary[dat$InUK==1]
salmin <- dat$SalaryMin[dat$InUK==1]
salmax <- dat$SalaryMax[dat$InUK==1]

# Create an indicator whether a salary is per hour or hourly
hourly<-grepl("per hour|hourly",salary,ignore.case=TRUE)

# Remove any embedded spaces.
salmin<-gsub(" ","",salmin)
salmax<-gsub(" ","",salmax)

# Count number of "Unspecified"" salaries
p1<-percent(length(salmin[grepl("Unspecified",salmin)])/n)
p2<-percent(length(salmax[grepl("Unspecified",salmax)])/n)
p<-max(p1,p2)

# Count the number of "Negotiable"" salaries
p1<-percent(length(salmin[grepl("Negotiable",salmin)])/n)
p2<-percent(length(salmax[grepl("Negotiable",salmax)])/n)
q<-max(p1,p2)
rm(p1,p2)

# Take entries that have a pound sign.
uksalmin<-salmin[grepl("£",salmin)]
uksalmax<-salmax[grepl("£",salmax)]
hourly<-hourly[grepl("£",salmin)|grepl("£",salmax)]

# Strip out the commas and the pound sign
uksalmin<-gsub("£|,","",uksalmin)
uksalmax<-gsub("£|,","",uksalmax)

# Convert entries into numbers
u<-as.numeric(uksalmin)
v<-as.numeric(uksalmax)

# Plot the two distributions side by side
# Plots appear to be on top of each other
xlimit<- 80000    # Salary to go up to.
ylimit<- 20000    # Maximum job count

hist(u,xlim=c(0,xlimit),ylim=c(0,ylimit),breaks=200,
     col=rgb(1,0,0,1/4), 
     xlab="Salary in £s",ylab="Number of jobs",main="",
	 cex.lab=0.75,cex.axis=0.75)

hist(v,xlim=c(0,xlimit),ylim=c(0,ylimit),breaks=200,
     col=rgb(0,0,1,1/4), 
     add=TRUE)

legend(60000,8000,c("Salary Min","Salary Max"),
       col=c(rgb(1,0,0,1/4),rgb(0,0,1,1/4)),pch=15,cex=0.75)

```

`r p` salaries are "*Unspecified*" and `r q` salaries are labelled as
"*Negotiable*".

The plot below plots the maximum vs the minimum salary.

```{r max_vs_min_salaries}

# Create a data frame with the salaries
hourly<-gsub(TRUE,"yes",hourly)
hourly<-gsub(FALSE,"no",hourly)
sal<-data.frame(salmin=u,salmax=v, hourly=hourly)

# Work out a linear model
mlin<-lm(sal$salmax ~ sal$salmin)

# Get the coefficients
c<-coefficients(mlin)

# Plot the data
ggplot(data=sal,aes(x=salmin,y=salmax))+
      geom_point(size=2,alpha=0.25)+
      labs(x="Salary Minimum",y="Salary Maximum") #+ geom_smooth()

# Fit a linear model
flm<-lm(v ~ u)
c<-coef(flm)

```

Can fit a linear model (`lm`) on the plot, which shows that 
the maximum salary is roughly 20% higher than the minimum salary:

```{r salary_lm}
cat(paste0("MaxSal = ",round(c[2],2)," * MinSal ",round(c[1],2)))
```

```{r hourly_salaries}
# ggplot(data=sal,aes(x=salmin[hourly],y=salmax[hourly],fill=hourly))+
#      geom_point(size=2)+
#     scale_x_continuous(limit=c(13500,14500))+
# scale_y_continuous(limit=c(13000,15000)) +
#     labs(x="Salary Minimum",y="Salary Maximum") 
  

# Remove variables no longer needed
rm(u,v,salmin,salmax,uksalmin,uksalmax,hourly)

```

## Salaries by subject

### Ranges

Can look at the salary spreads by subject. For an comprehensive
explanation of whisker plots see this
[article](http://flowingdata.com/2008/02/15/how-to-read-and-use-a-box-and-whisker-plot/),
to summarise:


> the median is the heavy line at the centre, the plot
> has a box round the 25-50% mark and 50-75% mark, the whiskers indicate
> the minimum and maximum and outliers, those that are some distance
> outside the upper or lower quartile. More precisely an outlier is any
> value that lies more than one and a half times the length of the box
> from either end of the box (extracted from
> [here](http://www.purplemath.com/modules/boxwhisk3.htm)). That is, if a
> data point is below Q1 – 1.5×IQR or above Q3 + 1.5×IQR, it is viewed
> as being too far from the central values to be reasonable.

First look at the salary minimums:

```{r uk_min_subject_boxplot}

# Another way of splitting produces a data frame:
# http://stackoverflow.com/questions/24595421/how-to-strsplit-data-frame-column-and-replicate-rows-accordingly

sub2<-data.frame(id=dat$JobId,
                 sub=dat$Subject,
                 sal=dat$Salary,
                 min=dat$SalaryMin,
                 max=dat$SalaryMax,
                 loc=dat$Location2,
                 con=dat$Contract,
                 uk=dat$InUK,
                 sfw=dat$SoftwareJob,
                 hours=dat$Hours,
                 stringsAsFactors = FALSE)

# Define any unspecified elements explicitly
sub2$sub[sub2$sub == ""] <- c("Unspecified")
sub2$con[sub2$con == ""] <- c("Unspecified")

# Split the subject field on the semicolon
tmp<-strsplit(sub2$sub,";")

# Find the length of each sublist
len<-sapply(tmp,length)

# Attach the length of the each list to the data frame. Can use this to 
# calculate partial contributions for jobs classified
sub2<-cbind.data.frame(sub2,len=len)

# Bind as new rows and replicate existing ones
tmp<-cbind.data.frame(name=unlist(tmp), row=rep(1:nrow(sub2), times=len),
                      stringsAsFactors=FALSE)

# Merge the rows between the sub2 and tmp data frames.
# The "-1" removes the "row" column from the tmp data frame.
sub2 <- merge(y=sub2, x=tmp, by.x="row", by.y="row.names", all.x=TRUE)[-1]

# Remove variables that are not going to be used any more.
rm(tmp,len)

# Remove the original subject column.
sub2<-sub2[,-c(3)]

# Actual contribution - fraction count, i.e. if there are 3 subjects
# as opposed to counting each one individually each will only contribute
# 1/3 of the value.
sub2$inv <- 1.0/sub2$len

# Process salaries - remove pound signs, spaces and any commas from
# the min or max salary.
sub2$min<-gsub(" |£|,","",sub2$min)
sub2$max<-gsub(" |£|,","",sub2$max)

# Need to remove any rows that have Unspecified of Negotiable salaries.
sub2<-sub2[-grep("Unspecified|Negotiable",sub2$min),]

# Convert salaries to numerical types.
sub2$min<-as.numeric(sub2$min)
sub2$max<-as.numeric(sub2$max)

# Split list into multiple graphs because there are too many subjects
l<-sort(unique(sub2$name))
n1<-length(l)
n2<-as.integer(n1/3)
sub3<-sub2[sub2$name %in% l[1:n2],]
sub4<-sub2[sub2$name %in% l[n2+1:n2*2],]
sub5<-sub2[sub2$name %in% l[(2*n2+1):n1],]

# Sort the frames alphabetically
sub3$name <- factor(sub3$name,levels = sort(unique(sub3$name),decreasing=TRUE))
sub4$name <- factor(sub4$name,levels = sort(unique(sub4$name),decreasing=TRUE))
sub5$name <- factor(sub5$name,levels = sort(unique(sub5$name),decreasing=TRUE))

# Boxplot of all the salary spreads by subject. 
ggplot(data=sub3,aes(x=name,y=min,fill=name)) +
   geom_boxplot(show.legend = FALSE) +
   labs(x="Subject",y="Salary minimum in pounds") +
   coord_flip()

ggplot(data=sub4,aes(x=name,y=min,fill=name)) +
   geom_boxplot(show.legend = FALSE) +
   labs(x="Subject",y="Salary minimum in pounds") +
   coord_flip()


ggplot(data=sub5,aes(x=name,y=min,fill=name)) +
   geom_boxplot(show.legend = FALSE) +
   labs(x="Subject",y="Salary minimum in pounds") +
   coord_flip()

```

where for the UK minimum salary distribution we have:

```{r min_summary}
summary(sub2$min[sub2$uk==1])
```

For the the salary maximums:

```{r uk_max_sub_boxplot}

ggplot(data=sub3,aes(x=name,y=max,fill=name)) +
      geom_boxplot() +
      labs(x="Subject",y="Salary maximum in pounds") +
      theme(legend.position="none") + coord_flip()

ggplot(data=sub4,aes(x=name,y=max,fill=name)) +
      geom_boxplot() +
      labs(x="Subject",y="Salary maximum in pounds") +
      theme(legend.position="none") + coord_flip()

ggplot(data=sub5,aes(x=name,y=max,fill=name)) +
      geom_boxplot() +
      labs(x="Subject",y="Salary maximum in pounds") +
      theme(legend.position="none") + coord_flip()


```

and for the UK max salary distribution:

```{r max_summary}
summary(sub2$max[sub2$uk==1])
```

### Salary density distributions by subject

One can look at the salary distributions per subject.

```{r salary_subject_densities, results="asis"}

# Number of entries
n<-length(sub2$name)

# Plot salary densities by subject.
# Loop over the subjects.
for( i in (unique(sub2$name)) ){

  # Create new data frame with the pertinent data for
  # the chosen subject.
  tmp<-sub2[grepl(i,sub2$name),]
  
  # Remove all columns except the min and max salaries.
  tmp<-tmp[,-c(1,2,3,6,7,8,9,10,11,12)] 
  
  # Ignore any subjects that do not have salary mins or maxes.
  if(length(tmp$min)==0|length(tmp$max)==0){ next}
  
  # Stack the columns of the data frame on top of each other with an
  # extra column indicating the original column name.
  oo<-stack(tmp)

  # Title composed of the subject appended by "Salary Density"
  title<-paste(i,"Salary Density")
  
  # Generate a graph
  p<- ggplot(data=oo, aes(values,fill=ind)) +
      geom_density(alpha=0.2) +
      ggtitle(title) +
      scale_fill_manual(values=c("red","blue"),name="Salary",
  	                    labels=c("Maximum","Minimum"))

  # Print a markdown header
  cat(paste("\n\n####",title,"\n\n"))
  
  # Print the graph
  print(p+labs(x="Salary",y="Density"))
  
  # Print a histogram of the graph as well - want to see how
  # these relate to the density plots.
  print(p+stat_bin(binwidth=500)+
          labs(x="Salary",y="Number of jobs"))

  # Add some information about the minimum salary number of entries
  cat(paste0("\n\nNumber of entries ",length(tmp$min)," (",
             percent(length(tmp$min)/n),
             ") and summary of the minimum salary for ",i,":\n\n"))
  
  # Add a statistical summary for the minimum salary
  cat(pander(summary(tmp$min)))
  
  # Add information about the maximum salary entries
  cat(paste0("\n\nNumber of entries ",length(tmp$max)," (",
             percent(length(tmp$max)/n),
             ") and summary of the maximum salary for ",i,":\n\n"))
  
  # Add a statistical about the minimum salary
  cat(pander(summary(tmp$max)))
  
}

# Remove variables no longer required
rm(tmp, p)

```

## Salary spreads by location

This uses the location identifier which is part of the job description
and aggregates the salary by the location. Salary minimum by location
(this is Location2). 

Note that "Australasian" salaries appear to have a higher than normal
median as the job publishers appear to be converting an Australian
dollars salary directly into pounds without doing a currency
conversion see this
[issue](https://github.com/softwaresaved/jobs-analysis/issues/21).
It's not clear to me whether they do this for some or all
jobs. Equally well other foreign currency jobs may be skewed this way.


```{r plot_salary_min_by_location}

# Sorting order of the locations (in reverse order)
locs = c("South East England","South West England", "London",
         "Midlands of England", "Northern England","Wales","Scotland",
         "Northern Ireland", "Republic of Ireland", "Europe",
		     "North, South & Central America", "Africa", "Australasia", 
         "Asia & Middle East",  "All Locations"
         )

# Boxplot of all the salary spreads by Location2. 
ggplot(data=sub2,aes(x=loc,y=min,fill=loc)) +
   geom_boxplot() +
   labs(x="Location",y="Salary minimum in pounds") +
   theme(legend.position="none") + coord_flip() +
   scale_x_discrete(limits=rev(locs))

```

and the salary maximum:

```{r plot_salary_max_by_location}

# Boxplot of all the salary spreads by Location2. 
ggplot(data=sub2,aes(x=loc,y=max,fill=loc)) +
   geom_boxplot() +
   labs(x="Location",y="Salary maximum in pounds") +
   theme(legend.position="none") + coord_flip()  +
   scale_x_discrete(limits=rev(locs))

```

## Salary density by location

```{r salary_densities_by_location, results="asis"}

# Number of entries
n<-length(sub2$name)

# Plot salary densities by subject.
# Loop round the subjects.
for( i in (unique(sub2$loc)) ){

  # Extract the pertinent data for subject
  tmp<-sub2[grepl(i,sub2$loc),]
  
  # Remove all columns except the min and max salaries
  tmp<-tmp[,-c(1,2,3,6,7,8,9,10,11,12)] 
  
  # Ignore any entries that do not have salary mins or maxes
  if(length(tmp$min)==0|length(tmp$max)==0){ next}
  
  # Stack the columns of the data frame on top of each other with an extra
  # column indicating the column name it came from.
  oo<-stack(tmp)

  # Title
  title<-paste("Salary Density for",i)
  
  # Do not use scientific notation
  options(scipen = 3)
  
  # Generate the graph
  p<- ggplot(data=oo, aes(values,fill=ind)) +
      geom_density(alpha=0.2) +
      labs(x="Salary",y="Density") +
      ggtitle(title) +
      scale_fill_manual(values=c("red","blue"),name="Salary",
  	                    labels=c("Maximum","Minimum"))

  # Print header
  cat(paste("\n\n###",title,"\n\n"))
  
  # Print the graph
  print(p)
  
  cat(paste0("\n\nNumber of entries ",length(tmp$min)," (",
             percent(length(tmp$min)/n),
             ") and summary of the minimum salary for ",i,":\n\n"))
  cat(pander(summary(tmp$min)))
  
  cat(paste0("\n\nNumber of entries ",length(tmp$max)," (",
             percent(length(tmp$min)/n),
             ") and summary of the maximum salary for ",i,":\n\n"))
  cat(pander(summary(tmp$max)))
  
}

# Remove variables no longer required
rm(tmp, p)

```


## Salary spread by contract type

```{r plot_salary_min_by_contract}

# Boxplot of all the salary spreads by contract. 
ggplot(data=sub2,aes(x=con,y=min,fill=con)) +
   geom_boxplot() +
   scale_y_continuous(limit=c(0,100000)) +
   labs(x="Contract type",y="Salary minimum in pounds") +
   theme(legend.position="none") + 
   coord_flip()

```


```{r plot_salary_max_by_contract}

# Boxplot of all the salary spreads by contract. 
ggplot(data=sub2,aes(x=con,y=max,fill=con)) +
   geom_boxplot() +
   scale_y_continuous(limit=c(0,100000)) +
   labs(x="Contract type",y="Salary maximum in pounds") +
   theme(legend.position="none") + 
   coord_flip()

```


## Salary density by contract type

```{r salary_densities_by_contract_type, results="asis"}

# Plot salary densities by subject.
# Loop round the subjects.
for( i in (unique(sub2$con)) ){

  # Extract the pertinent data for subject
  tmp<-sub2[grepl(i,sub2$con),]
  
  # Remove all columns except the min and max salaries
  tmp<-tmp[,-c(1,2,3,6,7,8,9,10,11,12)] 
  
  # Ignore any entries that do not have salary mins or maxes
  if(length(tmp$min)==0|length(tmp$max)==0){ next}
  
  # Stack the columns of the data frame on top of each other with an extra
  # column indicating the column name it came from.
  oo<-stack(tmp)

  # Title
  title<-paste("Salary Density for",i)
  
  # Do not use scientific notation
  options(scipen = 3)
  
  # Generate the graph
  p<- ggplot(data=oo, aes(values,fill=ind)) +
      geom_density(alpha=0.2) +
      labs(x="Salary",y="Density") +
      ggtitle(title) +
      scale_fill_manual(values=c("red","blue"),name="Salary",
  	                    labels=c("Maximum","Minimum"))
 #       scale_x_continuous(limit=c(0,100000)) +
 
  # Print header
  cat(paste("\n\n###",title,"\n\n"))
  
  # Print the graph
  print(p)
  
  cat(paste0("\n\nNumber of entries ",length(tmp$min),
             " and summary of the minimum salary for ",i,":\n\n"))
  cat(pander(summary(tmp$min)))
  
  cat(paste0("\n\nNumber of entries ",length(tmp$max),
             " and summary of the maximum salary for ",i,":\n\n"))
  cat(pander(summary(tmp$max)))
  
}

# Remove variables no longer required
rm(tmp, p)

```


# Subjects

## Overview

Some jobs come with a classification (subject area). Note that a job
can be classified into more than one subject area. In this case each
of those classifications will count as one towards each subject (over
counting).

```{r tabulate_subjects}

# Tabulate the number of subjects
subtab<-as.matrix(table(sub2$name))

# Total number of subjects
tot<-sum(as.numeric(subtab[,1]))

# Aggregate by categories, set the names
d<-setNames(aggregate(sub2$inv,by=list(sub2$name),FUN=sum),c("Role","Number"))

# Round off numbers
d$Number<-round(d$Number,2)

# Bind the new columns
subtab<-cbind(subtab,percent(subtab/tot),d$Number,percent(d$Number/tot))

# Provide names for the columns
colnames(subtab)<-c("Integral Count","Percentage",
                    "Fractional Count","Percentage")
pander(subtab,justify=c("left","left","left","left","left"),split.table=Inf)

```

Doing the same exercise for software jobs:

```{r sw_subjects}
# Tabulate the number of subjects
subtab<-as.matrix(table(sub2$name[sub2$sfw==1]))
   
# Aggregate by categories, set the names
d<-setNames(aggregate(sub2$inv[sub2$sfw==1],by=list(sub2$name[sub2$sfw==1]),FUN=sum),
            c("Role","Number"))

# Round off numbers
d$Number<-round(d$Number,2)

subtab<-cbind(subtab,percent(subtab/tot),d$Number,percent(d$Number/tot))

colnames(subtab)<-c("Integral SW Jobs Count","Percentage",
                    "Fractional SW Jobs Count","Percentage")

pander(subtab,style="rmarkdown",justify=c("left","left","left","left","left"),
       split.table=Inf)
```

# Location 2

Jobs come with their own aggregate location classification:

```{r locations}

# Grab the locations
m<-as.matrix(table(dat$Location2))

# Total number of locations
nlocs<-sum(m)

# Add a percentage column
m<-cbind(m,percent(m/nlocs))

# Provide column names (these become the table headers)
colnames(m)<-c("Location","Percentage")

# Print out using pander
pander(m,style="rmarkdown",justify=c("left","left","left"),split.table=Inf)
```

We can do the same thing for the currently identified software jobs
(percentages are relative to the total number of jobs).

```{r sw_locations}

# Grab the locations
m<-as.matrix(table(dat$Location2[dat$SoftwareJob==1]))

# Add a percentage column
m<-cbind(m,percent(m/nlocs))

# Provide column names (these become the table headers)
colnames(m)<-c("SW job Location","Percentage")

# Print out using pander
pander(m,style="rmarkdown",justify=c("left","left","left"),split.table=Inf)
```

# Roles

## Overview

Counting the roles in two ways - if a job is classified under more
than one role then the number of sub-classification will be converted
to a fractional count which will then be added to the total
classification count. The other version will count each job
classification as an integral job under each classification.


```{r role_descriptions}

# Create a new data frame
ro<-data.frame(role=dat$Role,stringsAsFactors = FALSE)

# Fill in any unspecified columns
ro$role[ro$role == ""]<-c("Unspecified")

# Split the subject field on the semicolon, produces a list
tmp<-strsplit(ro$role,";")

# Find the length of each sublist
len<-sapply(tmp,length)

# Attach the length of the each list to the data frame. Can use this to
# calculate partial contributions for jobs classified
ro<-cbind.data.frame(ro,len=len)

# Bind as new rows and replicate existing ones
tmp<-cbind.data.frame(name=unlist(tmp), row=rep(1:nrow(ro), times=len), 
                      stringsAsFactors=FALSE)

# Merge the rows between the sub2 and tmp data frames
ro <- merge(y=ro, x=tmp, by.x="row", by.y="row.names", all.x=TRUE)[-1]

# Remove the role column
ro<-ro[,-c(2)]

# Actual contribution
ro$inv<-1.0/ro$len

# Aggregate by categories, set the names
d<-setNames(aggregate(ro$inv,by=list(ro$name),FUN=sum),c("Role","Number"))

# Round off numbers
d$Number<-round(d$Number,2)

# Total number of jobs
n<-length(dat$JobId)

# convert the fractional count to a matrix 
m1<-as.matrix(d$Number)
m1<-cbind(m1,percent(m1[,1]/n))
rownames(m1)<-d$Role

# convert the raw count as a matrix
m2<-as.matrix(table(ro$name))
m2<-cbind(m2,percent(m2[,1]/n))

# Merge the two matrices
m<-merge(x=m1,y=m2,by="row.names")
colnames(m)<-c("Role","Fractional count","%","Integral count","%")

# Print out using pander
pander(m,style="rmarkdown",
       justify=c("left","left","left","left","left"),split.table=Inf)

# Remove data no longer required
rm(tmp,m,m1,m2)

```

# Hours

## Overview

```{r contract_hours}

# Fill in blank lines
dat$Hours[dat$Hours==""]<-"Unspecified"

# Work out the numbers
m<-as.matrix(table(dat$Hours))
n<-length(dat$Hours)
m<-cbind(m,percent(m/n))
colnames(m)<-c("Number of jobs","Percent of total")

# Tabulate
pander(m,style="rmarkdown", justify=c("left","right","right"),split.table=Inf)

# Remove entries not needed any more
rm(m,n)

```

# Contracts types

## Overview

```{r contract_types}

# Create a new data frame
con<-data.frame(contracts=dat$Contract)

# Remove embedded spaces
con$contracts<-gsub(" ","",con$contracts)

# Replace blanks with unspecified
con$contracts[con$contracts==""]<-c("Unspecified")

# Do a quick plot of the data
qplot(contracts, 
      data=con, 
      fill=contracts, 
      geom="bar",
      ylab="Number of Jobs",
      xlab="Contract Type") + scale_x_discrete(breaks=NULL)
```

Can look at the data:

```{r contract_table}

# Construct a table suitable for pander
# Create a matrix
mcon<-as.matrix(table(con$contracts))

# Attach a column of %ages
mcon<-cbind(mcon,percent(mcon/length(con$contracts)))

# Name the columns
colnames(mcon)<-c("Number of jobs","Percent") 

# Plot in a form suitable for direct wiki input
pander(mcon,style="rmarkdown",justify=c("left","left","left"))

```

## Subjects by contract type

Tabulate the number of subjects by contract type.

```{r contracts_by_subjects}

# Tabulate subject names against contract types.
pander(table(sub2$name,gsub(" ","",sub2$con)),
       justify=c("left","right","right","right","right"),
       style="rmarkdown",split.table=Inf)

```

Can look at the contract types by subject. 

```{r contract_by_subject_norm}

ggplot(data=sub3,aes(x=name,fill=con)) + 
  geom_bar(position="fill") + 
  xlab("Subject") + ylab("Percentage") +
  scale_y_continuous(labels = percent) + 
  coord_flip()

ggplot(data=sub4,aes(x=name,fill=con)) + 
  geom_bar(position="fill") + 
  xlab("Subject") + ylab("Percentage") +
  scale_y_continuous(labels = percent) + 
  coord_flip()

ggplot(data=sub5,aes(x=name,fill=con)) + 
  geom_bar(position="fill") + 
  xlab("Subject") + ylab("Percentage") +
  scale_y_continuous(labels = percent) + 
  coord_flip()



```

Looking at the actual numbers.

```{r plot_contract_types_by_subjects}

# Create a temporary data frame
sub6<-sub3
sub7<-sub4
sub8<-sub5

# Remove all columns except the name and contract columns
sub6<-sub6[,-c(2,3,4,5,6,8,9,10,11,12)]
sub7<-sub7[,-c(2,3,4,5,6,8,9,10,11,12)]
sub8<-sub8[,-c(2,3,4,5,6,8,9,10,11,12)]

# Plot the results
ggplot(data=sub6,aes(x=name,fill=con)) + 
  geom_bar() + xlab("Subject") + ylab("Number of jobs") +
  coord_flip()

ggplot(data=sub7,aes(x=name,fill=con)) + ylab("Number of jobs") +
  geom_bar() + xlab("Subject") + 
  coord_flip()

ggplot(data=sub8,aes(x=name,fill=con)) + ylab("Number of jobs") +
  geom_bar() + xlab("Subject") + 
  coord_flip()

# Remove the temporary variable
rm(sub3,sub4,sub5,sub6,sub7,sub8)
```

# Locations

## Jobs in the UK

```{r init_maps}

# Read in known locations
guk<-read.csv(file="../data/UKLocs.csv",
              stringsAsFactors = FALSE,
              strip.white = TRUE,
              col.names=c("name","lon","lat"))

# Remap some of the data (function in fileStuff.R)
dat<-cleanLocations(dat)

# Get the UK locations - split any comma separated values and give each
# their own entry. As a list can have a UK location and a non-UK location
# we need to clean these up. End up with a character array with all the known 
# UK locations and some non-UK places.
uklocs<-unlist(strsplit(dat$Location[dat$InUK==1],","))

# Now grab identified software jobs
ukswlocs<-unlist(strsplit(dat$Location[dat$InUK==1 & dat$SoftwareJob==1],","))

# Need to remove spaces after splitting and collapsing the list.
uklocs <- sub("^\\s+", "", uklocs) # Trim leading spaces
uklocs <- sub("\\s+$", "", uklocs) # Trim trailing spaces

ukswlocs <- sub("^\\s+", "", ukswlocs) # Trim leading spaces
ukswlocs <- sub("\\s+$", "", ukswlocs) # Trim trailing spaces

# Produce a list of all the unique locations.
uknames<-levels(factor(uklocs))
ukswnames<-levels(factor(ukswlocs))

# List all locations not in the UK or ambiguous terms (e.g. Nationwide or 
# Northern Ireland which is in the UK but not a specific location). These
# slip through after unsplitting the list.
notinuk <- c("Aachen", "All Locations","Ankara","Antarctic","Australia","Accra",
             "Bangalore","Bangladesh","Bangon","Barcelona","Beijing","Bergen","Bideford",
             "Brazil", "Budapest",
             "China","Chengdu","Colombia","Cork",
             "Dalian","Dar Es Salaam","Darmstadt","Durban","Dublin","Dijon",
             "Europe",
             "Fribourg",
             "Gaithersburg","Gambia","Geneva","Groningen",
             "Havana","Hebei","Home","Hong Kong","Hamburg","Home Based",
             "Haining",
             "Ireland",
             "Japan",
             "Kgs Lyngby","Kenya","Krakow",
             "Leuven","Lyon",
             "Malaysia","Masai Mara","Melbourne","Milano","Molndal",
             "Nationwide", "Nairobi","Netherlands","Northern Ireland","New York",
             "Paris",
             "Qatar",
             "Reduit","Rijswijk", "Roskilde",
             "San Fransisco","Santa Clara","Sevillle","Shanghai","Sierra Leone",
             "Singapore","Stockholm","Switzerland",
	           "Sri Lanka", "Stavanger",
             "Texas","Trento","Tucson","Toulouse",
             "Uganda","United Kingdom","Utrecht",
             "Virtual",
             "Wales","Wexford","Weihai",
             "Xiamen",
             "Zurich"
)

# Remove locations not in the UK
uknames <- uknames[! uknames %in% notinuk]

ukswnames <- ukswnames[! ukswnames %in% notinuk]

# Remove duplicates
uknames <- levels(factor(uknames))

ukswnames <- levels(factor(ukswnames))


# Grab any new unknown locations
unknowns<-uknames[! uknames %in% guk$name]

# Find any locations if we have got some new unknown places
if(length(unknowns)>0){
  
  # Query how many queries can be carried out. Can only do 2500
  # free queries per day.
  geocodeQueryCheck(userType="free")
  
  # Make the fact that we are using UK names explicit by appending
  # a ", UK" to every name when obtaining the lon & lat. Picking up
  # some aliases that are not in the UK. Of course, I am assuming 
  # that the British equivalent is meant mostly - could check the 
  # value in Location2.
  unknowns2<-gsub("(.*)","\\1, UK",unknowns)
  
  # This will create a data frame with the lon and lat of each location.
  # Appears to pull information from Google and has a restriction of a 
  # max of 2000 queries per day. Best to get the data and then to save
  # it. Next read from file and only query any unknown locations.
  guk2<-geocode(unknowns2) 
  
  # Attach the place names to the data frame.
  guk2$name<- unknowns
  
  # Merge the data frame of knowns and unknowns
  guk<-rbind(guk,guk2[c("name","lon","lat")])
  
  # Append the data frame if known location
  if(!(is.na(guk2$lon)|is.na(guk2$lat))){
    write.csv(guk[,c("name","lon","lat")],
              file="../data/UKLocs.csv",row.names = FALSE)
  }else{
    print(paste0("Unknown locations but can't auto generate lat/lon: ",
                 unknowns))  
  }
  
  # Remove variables we no longer need
  rm(unknowns,unknowns2,guk2)
  
}

# Now add the frequency of locations
ukswfreq<-table(ukswlocs[ukswlocs %in% ukswnames])
tmp<-data.frame(name=names(ukswfreq),num=as.numeric(ukswfreq))
guksw <- merge(guk,tmp,by="name")

ukfreq<-table(uklocs[uklocs %in% uknames])
tmp<-data.frame(name=names(ukfreq),num=as.numeric(ukfreq))
guk<-merge(guk,tmp,by="name")

# Remove variables
rm(tmp,ukswfreq,ukfreq)

# Get a UK-base map
#ukmap <- get_map(location = 'UK', 
#                 zoom = 5,
#                 maptype="watercolor",
#                 source="stamen",
#                 filename="ukmaptmp")
# cached using: saveRDS(ukmap,file="ukmapfile.rds")
ukmap<-readRDS(file="ukmapfile.rds")
```

Distribution of UK jobs:

```{r uk_jobs_map}
ggmap(ukmap, extent="panel") + 
      geom_point(data=guk,aes(x=lon,y=lat,size=num),col="red") +
      scale_size_area("Number of jobs",max_size=10)
```

## Software jobs in the UK

Examining UK software jobs only:

```{r uk_sw_jobs_map}

ggmap(ukmap, extent="panel") + 
  geom_point(data=guksw,aes(x=lon,y=lat,size=num),col="blue") +
  scale_size_area("Number of software jobs",max_size=10)

```

# Employers

## Distribution

There are a total of `r length(table(dat$Employer))` employers, 
`r length(table(dat$Employer[dat$InUK==1]))` 
of these are UK employers. The distribution
of jobs posted by each institution is fairly sharp.

```{r employer_distribution}

# Sorted frequency table of UK employers
emp<-sort(table(dat$Employer[dat$InUK==1]),decreasing = TRUE)

# Do the same thing for software jobs
swemp<-sort(table(dat$Employer[dat$SoftwareJob==1&dat$InUK==1]),
            decreasing = TRUE)
```

For all jobs we have:

```{r plot_emp_job_dist}
# Get an idea of the distribution
barplot(emp,col="red",ylab="Number of jobs",xlab="Institutions",
        axisnames = FALSE,xlim=c(0,200))

```

Only showing the first 200 institutions from `r length(emp)`
employers - the tail is very long. Similarly for software jobs:

```{r plot_swemp_job_dist}
# Get an idea of the distribution
barplot(swemp,col="blue",ylab="Number of software jobs",xlab="Institutions",
        axisnames = FALSE,xlim=c(0,100))

```

Only showing the top 200 employers of software jobs from a total of
`r length(swemp)` software job employers.

## Top 20 UK employers

### All jobs

The top 20 job posters are:

```{r all_job_posters}

# Total number of jobs
nj<-sum(emp)

# Only take the top 20 employers
emp2<-head(emp,20)

# Set up the data
m<-as.matrix(emp2)
m<-cbind(m,percent(as.numeric(emp2)/nj))
colnames(m)<-c("Number of Jobs","% of the total")

# Tabulate the results
pander(m,justify=c("left","left","left"))

# Remove variables no longer needed.
rm(emp2,m)

```

### Software jobs

The top 20 software job posters:

```{r sw_job_posters}

# Total number of jobs
snj<-sum(swemp)

# Only take the top 20 employers
emp2<-head(swemp,20)

# Set up the data
m<-as.matrix(emp2)
m<-cbind(m,percent(as.numeric(emp2)/nj))
m<-cbind(m,percent(as.numeric(emp2)/snj))
colnames(m)<-c("Number of Jobs","% of total","% of s/w jobs")

# Tabulate the results
pander(m,justify=c("left","left","left","left"),split.table=Inf)

# Remove variables no longer needed.
rm(emp2,m)

```

## Russell Group employers

### All jobs

Focus on the Russell group of Universities:

```{r russell_group}

# Define the Russell group of Universities
russell <- c("university of birmingham",
             "university of bristol",
             "university of cambridge",
             "cardiff university",
             "durham university",
             "university of edinburgh",
             "university of exeter",
             "university of glasgow",
             "imperial college london",
             "kings college london",
             "university of leeds",
             "university of liverpool",
             "london school of economics and political science",
             "the university of manchester",
             "newcastle university",
             "university of nottingham",
             "university of oxford",
             "queen mary university of london",
             "queens university belfast",
             "university of sheffield",
             "university of southampton",
             "university college london",
             "university of warwick",
             "university of york"
             )
```

```{r russell_group_jobs}
# Grab the entries that are in the Russell Group
emp2<-emp[grepl(paste0(russell,collapse="|"),names(emp))]

# Set up the data
m<-as.matrix(emp2)
m<-cbind(m,percent(as.numeric(emp2)/nj))
colnames(m)<-c("Number of Jobs","% of the total")

# Tabulate the results
pander(m,justify=c("left","left","left"))

```

### Russell group software jobs

```{r russell_group_swjobs}
# Grab the entries that are in the Russell Group
emp2<-swemp[grepl(paste0(russell,collapse="|"),names(swemp))]

# Set up the data
m<-as.matrix(emp2)
m<-cbind(m,percent(as.numeric(emp2)/nj))
m<-cbind(m,percent(as.numeric(emp2)/snj))
colnames(m)<-c("Number of Jobs","% of total","% of s/w jobs")

# Tabulate the results
pander(m,justify=c("left","left","left","left"),split.table=Inf)

```

# Terms

## Computer languages

Use text mining capabilities to examine some of the terms in the job
descriptions. Cannot follow the following computer languages easily:

* R, C - probably have a lot of false positives.
* asp.net will be transformed to `asp net` and the terms will be
  treated separately.

Not sure single letter terms make it through easily.

```{r define_terms}

# Load up some auxiliary routines
source("R/tmUtils.R")

# Define the terms we are interested in examining. 
myterms <- c(
              "c",
              "c++",
              "c#",
              "fortran",
              "genstat",
              "hadoop",
              "html",
              "java",
              "javascript",
              "jquery",
              "julia",
              "matlab",
              "nosql",
              "perl",
              "php",
              "python",
              "r",
              "sas",
              "spark",
              "spss",
              "stata",
              "sql"
             )
```

```{r create_corpus}

# Create Corpus - join the job name (or title) with the job description
myc<-Corpus(VectorSource(paste0(dat$Name,". ",dat$Description)))

# Clean the data up
myc <- cleanCorpus(myc)
```

```{r create_dtm}
# Create the document term matrix
dtm <- DocumentTermMatrix(myc, control=list(wordLengths=c(1,Inf)))

# Number of times the term is found across documents.
numterms<-col_sums(dtm)

# Number of documents a term appears in.
termindocs<-col_sums(dtm>0)
ntrms<-termindocs[myterms]

# Total number of jobs
n<-length(dat$JobId)

# Create a matrix
m<-matrix(ncol=3,data=c(numterms[myterms],ntrms,percent(ntrms/n)))
rownames(m)<-myterms
colnames(m)<-c("Number of occurrences","Number of jobs","% of jobs")

pander(m,justify=c("left","left","left","left"),split.table=Inf)

```

# Appendix A: generating this report

<!-- read input for this section from another file -->
```{r child="Report/Generating.Rmd"}
```

# Appendix B: terms

## Anatomy of a job

<!-- read content from another file -->
```{r child="Report/Anatomy.Rmd"}
```

# Appendix C: To do items

<!-- List of items that need to be processed -->
```{r child="Report/ToDo.Rmd"}
```


